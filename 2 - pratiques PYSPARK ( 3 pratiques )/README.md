# 📘 Table des Matières
---
### 🚀 **Pratique 1 : Introduction à PySpark**
- **Démarrez avec PySpark !** Apprenez à installer PySpark, à initialiser `SparkContext` et à charger votre premier fichier texte. Explorez les possibilités de traitement de données avec Spark.

### 🌟 **Pratique 2 : Manipulations de RDD**
- **Approfondissez vos connaissances sur les RDD !** Maîtrisez les transformations et les actions clés de PySpark, telles que `flatMap`, `map`, `filter`, et `distinct`. Découvrez comment ces méthodes permettent de traiter et de transformer les données de manière efficace.

  - **Création de RDD :** Apprenez à créer des RDD à partir de variables, d'autres RDD ou de datasets externes.
  - **Transformations :** Explorez `map` pour appliquer des fonctions à chaque élément, `flatMap` pour transformer chaque entrée en plusieurs sorties, et `filter` pour sélectionner les données pertinentes.
  - **Actions :** Utilisez `collect` pour récupérer vos données, découvrez comment compter les éléments avec `count`, sommer des valeurs avec `sum`, et trouver des valeurs maximales ou minimales.

### 🔍 **Pratique 3 : Analyse de Données avec Spark SQL**
- **Explorez Spark SQL !** Utilisez Spark SQL pour effectuer des analyses complexes sur des données structurées. Chargez des données, créez des vues temporaires et exécutez des requêtes SQL pour analyser les données de marché.

  - **Chargement des Données :** Apprenez à charger des données à partir de fichiers CSV dans DataFrame Spark pour une manipulation facile.
  - **Requêtes SQL :** Effectuez des analyses de marché en calculant des différences de prix, en trouvant les volumes maximaux et minimaux, et en analysant les prix moyens d'ouverture par année ou les volumes par mois.
  - **Visualisation :** Découvrez comment intégrer PySpark avec des bibliothèques de visualisation comme Matplotlib pour afficher les tendances et les insights de vos analyses.

Ces pratiques vous offrent une vue d'ensemble solide pour commencer à travailler avec PySpark, de la manipulation basique des données à des analyses plus complexes en utilisant Spark SQL.

---

# Résumé 
---
# 📘 Table des Matières

### 🚀 **Pratique 1 : Les Bases de PySpark**
- **Explorez !** Initiation à PySpark et création de SparkContext.

### 🌟 **Pratique 2 : RDD et leurs Méthodes**
- **Plongez plus profond !** Découverte des méthodes RDD telles que `map`, `flatmap`, `filter`, et `distinct`.

### 🔍 **Pratique 3 : Actions sur les RDD**
- **Actionnez !** Utilisation des actions RDD comme `count`, `sum`, `max`, `min`, et `mean` pour analyser les données.

### 📊 **Pratique 4 : Analyse de Marché avec Spark SQL**
- **Analysez !** Exploration de Spark SQL pour analyser les données du marché boursier et visualisation des résultats.


---
# English version
---
# 📝 README.md

## 🚀 **Introduction**

Welcome to our PySpark tutorial series! Here we explore the power of PySpark through three practical exercises. Each exercise is designed to enhance your understanding of PySpark's capabilities, from basic operations to analyzing market trends.

## 📘 Table of Contents

### 🛠 **Pratique 1: Basics and RDD Operations**

- **Setup:** Learn how to install PySpark and set up your environment.
- **RDD Creation:** Discover how to create Resilient Distributed Datasets (RDDs) from various sources.
- **Transformations:** Explore transformation functions like `map`, `flatMap`, `filter`, and `distinct` to manipulate RDDs.
- **Actions:** Understand action functions such as `count`, `sum`, `max`, `min`, and `mean` to compute statistics.

### 🔧 **Pratique 2: Advanced RDD Techniques**

- **Advanced RDD Operations:** Dive deeper into RDDs with methods of creating RDDs from variables, other RDDs, or external datasets.
- **Transformation Functions:** Get hands-on with `map`, `flatMap`, and their use cases.
- **Filtering and Distinct:** Learn to filter data and remove duplicates to clean your datasets.
- **Actions Overview:** Review actions like `count`, `sum`, `max`, and `mean` to analyze your data.

### 📈 **Pratique 3: Market Volume Analysis**

- **Market Data Analysis:** Set up PySpark to analyze stock market data.
- **Data Loading and Viewing:** Load market data from a CSV file and view it using Spark SQL.
- **SQL Queries with PySpark:** Perform SQL queries to analyze opening and closing prices, calculate price differences, and explore volume trends.
- **Visualization:** Although not fully covered, hints are given towards using matplotlib for data visualization.

## 📚 **Learning Outcomes**

By following these practices, you'll gain a solid foundation in PySpark, from basic data manipulation to performing complex analyses on large datasets. You'll learn to leverage Spark's distributed data processing capabilities to uncover insights from real-world data, such as stock market trends.

### 🎓 **Skills Acquired**

- PySpark installation and setup
- RDD creation and manipulation
- Understanding of transformation and action operations
- Ability to perform complex SQL queries on datasets
- Basics of data visualization with PySpark

## 🔍 **Conclusion**

This README provides a structured path to mastering PySpark through practical exercises. Whether you're analyzing text data or uncovering trends in the stock market, PySpark offers a robust and scalable approach to handling big data.
