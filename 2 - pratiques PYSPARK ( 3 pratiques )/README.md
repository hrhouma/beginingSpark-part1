# üìò Table des Mati√®res

### üöÄ **Pratique 1 : Introduction √† PySpark**
- **D√©marrez avec PySpark !** Apprenez √† installer PySpark, √† initialiser `SparkContext` et √† charger votre premier fichier texte. Explorez les possibilit√©s de traitement de donn√©es avec Spark.

### üåü **Pratique 2 : Manipulations de RDD**
- **Approfondissez vos connaissances sur les RDD !** Ma√Ætrisez les transformations et les actions cl√©s de PySpark, telles que `flatMap`, `map`, `filter`, et `distinct`. D√©couvrez comment ces m√©thodes permettent de traiter et de transformer les donn√©es de mani√®re efficace.

  - **Cr√©ation de RDD :** Apprenez √† cr√©er des RDD √† partir de variables, d'autres RDD ou de datasets externes.
  - **Transformations :** Explorez `map` pour appliquer des fonctions √† chaque √©l√©ment, `flatMap` pour transformer chaque entr√©e en plusieurs sorties, et `filter` pour s√©lectionner les donn√©es pertinentes.
  - **Actions :** Utilisez `collect` pour r√©cup√©rer vos donn√©es, d√©couvrez comment compter les √©l√©ments avec `count`, sommer des valeurs avec `sum`, et trouver des valeurs maximales ou minimales.

### üîç **Pratique 3 : Analyse de Donn√©es avec Spark SQL**
- **Explorez Spark SQL !** Utilisez Spark SQL pour effectuer des analyses complexes sur des donn√©es structur√©es. Chargez des donn√©es, cr√©ez des vues temporaires et ex√©cutez des requ√™tes SQL pour analyser les donn√©es de march√©.

  - **Chargement des Donn√©es :** Apprenez √† charger des donn√©es √† partir de fichiers CSV dans DataFrame Spark pour une manipulation facile.
  - **Requ√™tes SQL :** Effectuez des analyses de march√© en calculant des diff√©rences de prix, en trouvant les volumes maximaux et minimaux, et en analysant les prix moyens d'ouverture par ann√©e ou les volumes par mois.
  - **Visualisation :** D√©couvrez comment int√©grer PySpark avec des biblioth√®ques de visualisation comme Matplotlib pour afficher les tendances et les insights de vos analyses.

Ces pratiques vous offrent une vue d'ensemble solide pour commencer √† travailler avec PySpark, de la manipulation basique des donn√©es √† des analyses plus complexes en utilisant Spark SQL.