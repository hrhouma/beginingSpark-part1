# ğŸ“˜ Table des MatiÃ¨res
---
### ğŸš€ **Pratique 1 : Introduction Ã  PySpark**
- **DÃ©marrez avec PySpark !** Apprenez Ã  installer PySpark, Ã  initialiser `SparkContext` et Ã  charger votre premier fichier texte. Explorez les possibilitÃ©s de traitement de donnÃ©es avec Spark.

### ğŸŒŸ **Pratique 2 : Manipulations de RDD**
- **Approfondissez vos connaissances sur les RDD !** MaÃ®trisez les transformations et les actions clÃ©s de PySpark, telles que `flatMap`, `map`, `filter`, et `distinct`. DÃ©couvrez comment ces mÃ©thodes permettent de traiter et de transformer les donnÃ©es de maniÃ¨re efficace.

  - **CrÃ©ation de RDD :** Apprenez Ã  crÃ©er des RDD Ã  partir de variables, d'autres RDD ou de datasets externes.
  - **Transformations :** Explorez `map` pour appliquer des fonctions Ã  chaque Ã©lÃ©ment, `flatMap` pour transformer chaque entrÃ©e en plusieurs sorties, et `filter` pour sÃ©lectionner les donnÃ©es pertinentes.
  - **Actions :** Utilisez `collect` pour rÃ©cupÃ©rer vos donnÃ©es, dÃ©couvrez comment compter les Ã©lÃ©ments avec `count`, sommer des valeurs avec `sum`, et trouver des valeurs maximales ou minimales.

### ğŸ” **Pratique 3 : Analyse de DonnÃ©es avec Spark SQL**
- **Explorez Spark SQL !** Utilisez Spark SQL pour effectuer des analyses complexes sur des donnÃ©es structurÃ©es. Chargez des donnÃ©es, crÃ©ez des vues temporaires et exÃ©cutez des requÃªtes SQL pour analyser les donnÃ©es de marchÃ©.

  - **Chargement des DonnÃ©es :** Apprenez Ã  charger des donnÃ©es Ã  partir de fichiers CSV dans DataFrame Spark pour une manipulation facile.
  - **RequÃªtes SQL :** Effectuez des analyses de marchÃ© en calculant des diffÃ©rences de prix, en trouvant les volumes maximaux et minimaux, et en analysant les prix moyens d'ouverture par annÃ©e ou les volumes par mois.
  - **Visualisation :** DÃ©couvrez comment intÃ©grer PySpark avec des bibliothÃ¨ques de visualisation comme Matplotlib pour afficher les tendances et les insights de vos analyses.

Ces pratiques vous offrent une vue d'ensemble solide pour commencer Ã  travailler avec PySpark, de la manipulation basique des donnÃ©es Ã  des analyses plus complexes en utilisant Spark SQL.

---

# RÃ©sumÃ© 
---
# ğŸ“˜ Table des MatiÃ¨res

### ğŸš€ **Pratique 1 : Les Bases de PySpark**
- **Explorez !** Initiation Ã  PySpark et crÃ©ation de SparkContext.

### ğŸŒŸ **Pratique 2 : RDD et leurs MÃ©thodes**
- **Plongez plus profond !** DÃ©couverte des mÃ©thodes RDD telles que `map`, `flatmap`, `filter`, et `distinct`.

### ğŸ” **Pratique 3 : Actions sur les RDD**
- **Actionnez !** Utilisation des actions RDD comme `count`, `sum`, `max`, `min`, et `mean` pour analyser les donnÃ©es.

### ğŸ“Š **Pratique 4 : Analyse de MarchÃ© avec Spark SQL**
- **Analysez !** Exploration de Spark SQL pour analyser les donnÃ©es du marchÃ© boursier et visualisation des rÃ©sultats.


---
# English version
---
# ğŸ“ README.md

## ğŸš€ **Introduction**

Welcome to our PySpark tutorial series! Here we explore the power of PySpark through three practical exercises. Each exercise is designed to enhance your understanding of PySpark's capabilities, from basic operations to analyzing market trends.

## ğŸ“˜ Table of Contents

### ğŸ›  **Pratique 1: Basics and RDD Operations**

- **Setup:** Learn how to install PySpark and set up your environment.
- **RDD Creation:** Discover how to create Resilient Distributed Datasets (RDDs) from various sources.
- **Transformations:** Explore transformation functions like `map`, `flatMap`, `filter`, and `distinct` to manipulate RDDs.
- **Actions:** Understand action functions such as `count`, `sum`, `max`, `min`, and `mean` to compute statistics.

### ğŸ”§ **Pratique 2: Advanced RDD Techniques**

- **Advanced RDD Operations:** Dive deeper into RDDs with methods of creating RDDs from variables, other RDDs, or external datasets.
- **Transformation Functions:** Get hands-on with `map`, `flatMap`, and their use cases.
- **Filtering and Distinct:** Learn to filter data and remove duplicates to clean your datasets.
- **Actions Overview:** Review actions like `count`, `sum`, `max`, and `mean` to analyze your data.

### ğŸ“ˆ **Pratique 3: Market Volume Analysis**

- **Market Data Analysis:** Set up PySpark to analyze stock market data.
- **Data Loading and Viewing:** Load market data from a CSV file and view it using Spark SQL.
- **SQL Queries with PySpark:** Perform SQL queries to analyze opening and closing prices, calculate price differences, and explore volume trends.
- **Visualization:** Although not fully covered, hints are given towards using matplotlib for data visualization.

## ğŸ“š **Learning Outcomes**

By following these practices, you'll gain a solid foundation in PySpark, from basic data manipulation to performing complex analyses on large datasets. You'll learn to leverage Spark's distributed data processing capabilities to uncover insights from real-world data, such as stock market trends.

### ğŸ“ **Skills Acquired**

- PySpark installation and setup
- RDD creation and manipulation
- Understanding of transformation and action operations
- Ability to perform complex SQL queries on datasets
- Basics of data visualization with PySpark

## ğŸ” **Conclusion**

This README provides a structured path to mastering PySpark through practical exercises. Whether you're analyzing text data or uncovering trends in the stock market, PySpark offers a robust and scalable approach to handling big data.
