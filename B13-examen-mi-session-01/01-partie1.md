
1. b) Resilient Distributed Dataset

Justification : RDD signifie Resilient Distributed Dataset. C'est l'abstraction de données fondamentale de Spark, représentant une collection distribuée et tolérante aux pannes d'éléments.

2. c) Les DataFrames sont optimisés pour les performances.
   d) Les RDDs ne supportent pas les opérations SQL.

Justification : Ces deux réponses sont correctes. Les DataFrames sont optimisés pour les performances grâce à leur structure tabulaire et leur schéma. Les RDDs ne supportent pas nativement les opérations SQL, contrairement aux DataFrames.

3. c) spark.read.csv()

Justification : La méthode spark.read.csv() est utilisée spécifiquement pour lire des fichiers CSV en tant que DataFrames dans Spark.

4. c) Charger les données dans la mémoire pour une utilisation future.

Justification : La méthode cache() permet de stocker un RDD ou DataFrame en mémoire pour accélérer les accès futurs.

5. c) Scala

Justification : Scala est le langage de programmation principal utilisé pour écrire des applications Spark. Spark a été développé en Scala et la majorité de son code source est en Scala.

6. b) val x = 10

Justification : En Scala, le mot-clé "val" est utilisé pour déclarer des variables immuables.

7. b) def add(x: Int, y: Int): Int = x + y

Justification : La syntaxe correcte pour déclarer une fonction en Scala utilise le mot-clé "def".

8. a) class

Justification : Le mot-clé "class" est utilisé pour définir une classe en Scala.

9. a) list.foreach(println)

Justification : La méthode foreach() est couramment utilisée pour itérer sur les éléments d'une collection en Scala.

10. c) List

Justification : La méthode map() appliquée à une List retourne une nouvelle List contenant les résultats de l'application de la fonction à chaque élément.
