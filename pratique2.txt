//********************************************************************************
//********************************************************************************
//PRATIQUE 2 - RDD ET AUTRES OPÉRATIONS DE BASE  
//********************************************************************************
//********************************************************************************

//(Section 01/05) FONCTION SAMPLE
val varsample= sc.parallelize( 1 to 100)
varsample.sample(true, 0.2, 5).collect


// Le premier paramètre détermine si nous aurons des doublons ou non 
// Le deuxième parametre : taille de l'echantillon ( vous pouvez verifier avec la fonction count
// Le troisieme parametre est le seed pour reproduire le même chantillon si nous désirons répéter les mêmes expériences 
varsample.count


//(Section 02/05) OPÉRATIONS RELATIONELLES
//2.1. JOINTURE 
//2.2. TRANFORMATIONS
//2.3. UNION-INTERSECTION-SUBSTRACT-
//2.4. DISTINCT - CARTESIAN- COGROUP
//2.5. JOIN-RIGHTOUTERJOIN-LEFTOUTERJOIN
//2.6. ACTIONS
//2.7. TAKEORDERED
//2.8. Finalement, nous allons voir 2 tranformations (keyBy, coalese) 


//tranformation union
val rdd1= sc.parallelize( List(1,2,3,4,5))
val rdd2= sc.parallelize( List(1,3,6,7))
rdd1.union(rdd2).collect

//tranformation intersection
rdd1.intersection(rdd2).collect

//tranformation substract
rdd1.subtract(rdd2).collect
rdd2.subtract(rdd1).collect

//tranformation cartesian
rdd1.collect
rdd2.collect 
rdd1.cartesian(rdd2).collect 

//tranformation distinct
val rdd3 = sc.parallelize( List(1,3,6,3,1,7,8,9))
rdd3.distinct().collect

//tranformation cogroup (comme le full outer join)
val rdd4 = sc.parallelize( Array(("A","1"),("B","2"), ("C","3"), ("D","4") ))
val rdd5 = sc.parallelize( Array(("A","a"),("B","b"), ("C","c"), ("D","d") ))
rdd4.cogroup(rdd5).collect
rdd5.cogroup(rdd4).collect
val rdd6 = sc.parallelize( Array(("A","a"),("C","c")))
rdd4.cogroup(rdd6).collect

//QUESTION : si on inverse clés valeurs dans l'exemple précédant
val rdd4f = sc.parallelize( Array(("1","A"),("2","B"), ("3","C"), ("4","D") ))
rdd4f.cogroup(rdd5).collect

//JOIN - RIGHTOUTERJOIN-LEFTOUTERJOIN
rdd4.collect
rdd5.collect 
rdd4.join(rdd5).collect

rdd4.collect
rdd6.collect
rdd4.join(rdd6).collect
rdd4.rightOuterJoin(rdd6).collect
rdd4.leftOuterJoin(rdd6).collect
rdd4.fullOuterJoin(rdd6).collect
rdd4.cogroup(rdd6).collect

//ACTION TAKEORDERED
sc.parallelize(List(1,2,3,4,5)).top(1)
sc.parallelize(List(1,2,3,4)).top(1)
sc.parallelize(List(1,12,5,3,4)).top(1)
sc.parallelize(List(1,12,5,3,4)).top(2)
sc.parallelize(List(1,12,5,3,4)).takeOrdered(1)
sc.parallelize(List(1,12,5,-3,4)).takeOrdered(1)
sc.parallelize(List(1,12,5,-3,4)).takeOrdered(2)
sc.parallelize(List(1,12,5,-3,4)).takeOrdered(3)


//(Section 03/05) LES OPÉRATIONS DATA STRUCTURE
//3.1. Nous allons voir 2 tranformations (keyBy, coalese) 
//3.2. Action (saveAsTextFile)
//3.3. La fonction coalese permet de créer une paire RDD (une paire pour chaque élément dans le RDD d'origine).
//3.4. La clé de la paire est calculée via une fonction qui est fournie par l'utilisateur


//keyBy
val rdd1 = sc.parallelize(List("1 val1","2 val2","3 val3","4 val4"))
rdd1.map(x=>(x.split(" ")(0),x.split(" ")(1))).collect
rdd1.keyBy(x=>x.split(" ")(0)).collect
rdd1.map(x=>(x.split(" ")(0),x)).collect
rdd1.map(x=>(x.split(" ")(0),x.split(" ")(1))).collect


//coalese
val rdd3 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10,11),5)
rdd3.glom.collect 
rdd3.coalesce(3).glom.collect  
rdd3.coalesce(7).glom.collect  

//action saveAsTextFile
val rdd5 = sc.parallelize(List("toto tata titi toto tutu tata"))
val rddsave = rdd5.flatMap (l => l.split(" ")).map( w => (w,1)).reduceByKey(_+_)
rddsave.saveAsTextFile("resultat1")


//(Section 04/05) LES OPÉRATIONS DATA STRUCTURE

//EXERCICE#8. Analyse de Fréquence des Mots avec Spark RDD

Objectif : Cet exercice vise à pratiquer les opérations de base sur 
les RDD (Resilient Distributed Datasets) dans Spark pour analyser 
la fréquence des mots dans différents ensembles de données. 
Vous utiliserez les méthodes flatMap, map, et reduceByKey pour 
compter combien de fois chaque mot apparaît dans les données.

Contexte : Vous disposez de trois ensembles de chaînes de caractères 
représentant des phrases ou des groupes de mots. Votre tâche est 
de calculer la fréquence de chaque mot dans ces ensembles en utilisant Spark.

Données :
- Ensemble 1 : List("toto tata titi toto tutu tata")
- Ensemble 2 : List("toto tata", "titi toto", "tutu tata")
- Ensemble 3 : List("toto tata titi", "toto tutu tata")

Instructions :

1. Initialisation de RDD : Pour chaque ensemble de données, initialisez un RDD en utilisant la méthode parallelize de SparkContext (sc).

2. Transformation et Action :
   - Étape A : Utilisez flatMap pour séparer les mots dans chaque ensemble de données. Chaque mot doit être traité comme un élément distinct.
   - Étape B : Appliquez map pour transformer chaque mot en une paire clé-valeur, où la clé est le mot et la valeur est 1.
   - Étape C : Utilisez reduceByKey pour additionner les valeurs associées à chaque clé, ce qui donne la fréquence de chaque mot dans l'ensemble.

3. Résultats : Pour chaque RDD (rdd5, rdd6, rdd7), affichez les résultats finaux sous forme de paires clé-valeur, où la clé est un mot et la valeur est sa fréquence dans l'ensemble de données correspondant.

Questions :

- Comparez les résultats obtenus pour les trois ensembles de données. 
Qu'observez-vous concernant la distribution des mots ?
- Expliquez l'impact de la structure des données initiales (une seule chaîne vs plusieurs chaînes) 
sur le résultat final.

Conseil : Assurez-vous d'avoir une instance Spark fonctionnelle et d'avoir initialisé SparkContext avant de commencer cet exercice. Utilisez les opérations RDD de Spark pour accomplir les tâches demandées et pour manipuler les données.

Bonne chance et amusez-vous bien à explorer les données avec Spark !

val rdd5 = sc.parallelize(List("toto tata titi toto tutu tata"))
val rdd6 = sc.parallelize (List("toto tata","titi toto","tutu tata"))
val rdd7 = sc.parallelize (List("toto tata titi","toto tutu tata"))

rdd5.flatMap (l => l.split(" ")).map( w => (w,1)).reduceByKey(_+_)
rdd6.flatMap (l => l.split(" ")).map( w => (w,1)).reduceByKey(_+_)
rdd7.flatMap (l => l.split(" ")).map( w => (w,1)).reduceByKey(_+_)





//(Section 05/05) fullOuterJoin VS cogroup

//EXERCICE#9.

Question 1: Pourquoi les résultats des opérations cogroup et join 
sur des RDDs dans Apache Spark sont-ils représentés différemment, 
l'un utilisant CompactBuffer dans un tuple (Iterable[String], Iterable[String]) 
et l'autre affichant directement les valeurs dans un tuple (String, String)?

Question 2: Quelle est la différence entre l'utilisation de fullOuterJoin 
et cogroup sur des RDDs dans Apache Spark, en termes de type de résultat 
retourné, comme illustré par les résultats 
Array[(String, (Option[String], Option[String]))] pour fullOuterJoin 
et Array[(String, (Iterable[String], Iterable[String]))] pour cogroup?



//tranformation cogroup (comme le full outer join)

val rdd4 = sc.parallelize( Array(("A","1"),("B","2"), ("C","3"), ("D","4") ))
val rdd6 = sc.parallelize( Array(("A","a"),("C","c")))
rdd4.cogroup(rdd6).collect


rdd4.fullOuterJoin(rdd6).collect
rdd4.cogroup(rdd6).collect


Différences:
res126: Array[(String, (Option[String], Option[String]))] = Array((A,(Some(1),Some(a))), (B,(Some(2),None)), (C,(Some(3),Some(c))), (D,(Some(4),None)))
res127: Array[(String, (Iterable[String], Iterable[String]))] = Array((A,(CompactBuffer(1),CompactBuffer(a))), (B,(CompactBuffer(2),CompactBuffer())), (C,(CompactBuffer(3),CompactBuffer(c))), (D,(CompactBuffer(4),CompactBuffer())))


https://github.com/hrhouma/beginingSpark-part1/blob/main/CoGroupPartie1.md
https://github.com/hrhouma/beginingSpark-part1/blob/main/CoGroupePartie2.md